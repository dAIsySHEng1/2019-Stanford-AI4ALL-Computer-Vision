{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and validation homework\n",
    "\n",
    "In this homework, we will cover how to implement the regularization techniques we learned in class:\n",
    "- Early Stopping\n",
    "- Weight Decay (L2 Regularization)\n",
    "- Dropout\n",
    "- Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\" Creates a new convolutional neural network and returns it \"\"\"\n",
    "    seed(1)\n",
    "    input_shape = (32, 32, 3)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing\n",
    "\n",
    "This code block loads our dataset and does some initial preprocessing. We are using the `MNIST` dataset of hand written digits. You can read about MNIST here: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      " 15081472/170498071 [=>............................] - ETA: 1:22"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = cifar10.load_data()\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 150\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "num_train = 1000\n",
    "num_val = 500  # We choose such a large validation set to illustrate the effects of overfitting without noise\n",
    "\n",
    "x_train = x_train[:num_train]\n",
    "y_train = y_train[:num_train]\n",
    "x_val = x_val[:num_val]\n",
    "y_val = y_val[:num_val]\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_train /= 255\n",
    "x_val /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_val.shape[0], 'validation samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# This function will print the training\n",
    "def evaluate_acc(model):\n",
    "    train_score = model.evaluate(x_train, y_train, verbose=0)\n",
    "    print('Train loss:', train_score[0])\n",
    "    print('Train accuracy:', train_score[1])\n",
    "    val_score = model.evaluate(x_val, y_val, verbose=0)\n",
    "    print('Validation loss:', val_score[0])\n",
    "    print('Validation accuracy:', val_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No regularization\n",
    "First, lets just try seeing how the model performs with no regularization. We are going to train for 150 epochs on \n",
    "\n",
    "This should take around 3-5 minutes to train. Go take a break and grab a snack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True)\n",
    "evaluate_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "With no regularization, you should get training accuracy around 0.783, and validation accuracy around 0.37. This is a pretty big gap, one which we cannot improve with just training. Let's first try to add early stopping! Add here some code that would run `model.fit` using early stopping.\n",
    "\n",
    "Hints: \n",
    "Check out how EarlyStopping is implemented in Keras and how you would go about adding it to your model here: https://keras.io/callbacks/.\n",
    "You may actually get worse performance initially by adding EarlyStopping. Try tuning the `patience` argument (read up on it in the keras docs). However, while it makes training faster, you likely won't see much improvement in validation accuracy (I get around 2% increase in validation accuracy). Note that while training accuracy may be lower, your validation accuracy will be higher. This is because the model is not super overfit to the training data yet, and has learned some understandable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "'''\n",
    "Your code goes here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Decay\n",
    "\n",
    "Early stopping has helped us not train longer than we need to, but in this case doesn't help much with validation accuracy. This means that the gap between validation and training accuracy is always large, and there are never any \"peaky\" moments where the distance is decreased. Let's next try to add L2 Weight Decay to our model.\n",
    "\n",
    "Hint: As example code, you can use the following code to declare a Dense layer with 64 hidden neurons and l2 regularization with weight decay strength 0.01. You can add this to *all* of the *learnable* layers in the model in `create_model`. Check out the documentation on regularizers here: https://keras.io/regularizers/\n",
    "\n",
    "```\n",
    "from keras import regularizers\n",
    "\n",
    "Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "Add it now to the model in `create_model`, and retrain it here to see your new results. Note that you can tweak your weight decay strength as a hyperparameter. However, I have personally done this and found best results with `0.01`. Can you see if you can outperform this? I personally see about a 1% improvement, and a much smaller gap between training accuracy and validation accuracy. This is due to the fact that while we are less overfit to our training data, we still learning valuable features of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_weight_decay():\n",
    "    \"\"\" Creates a new convolutional neural network with weight decay and returns it \"\"\"\n",
    "    seed(1)\n",
    "    input_shape = (32, 32, 3)\n",
    "    ### Beginning of your code\n",
    "    ''' Add a deep CNN model using weight decay here '''\n",
    "    ### End of your code\n",
    "    \n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run your model here and see how well you do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=)#TODO: Add your favorite patience value here)\n",
    "model = create_model_weight_decay()\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks=[early_stopping])\n",
    "evaluate_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "We next add dropout layers to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dropout():\n",
    "    \"\"\" Creates a new convolutional neural network with weight decay and returns it \"\"\"\n",
    "    seed(1)\n",
    "    input_shape = (32, 32, 3)\n",
    "    \n",
    "    ### Beginning of your code\n",
    "    ''' Add a deep CNN model using weight decay here '''\n",
    "    ### End of your code\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=)#TODO: Add your favorite patience value here)\n",
    "model = create_model_dropout()\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks=[early_stopping])\n",
    "evaluate_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get around 41% validation accuracy using dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
